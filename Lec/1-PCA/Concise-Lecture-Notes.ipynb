{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af90b7d5",
   "metadata": {},
   "source": [
    "# Review of Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85439dd0",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a147483",
   "metadata": {},
   "source": [
    "1. CDF always exists.\n",
    "\n",
    "2. Definition of PDF: $f_X(x)$ is said to be a pdf of r.v. X if $P(X\\le x)=\\int_{-\\infty}^x f_X(x)dx$\n",
    "\n",
    "3. PDF/PMF may not exist. (e.g. r.v.'s which are neither continuous nor discrete.)\n",
    "\n",
    "4. PDF is NOT unique. (因為單點機率=0)\n",
    "\n",
    "     e.g. Consider $f_X(x)$ to be a normal pdf.\n",
    "     \n",
    "     Define $\\tilde f_X(x)=\\begin{cases}f_X(x) & \\text{ if } x\\ne0\\\\ e^{10} & \\text{ if } x=0\\end{cases}$\n",
    "\n",
    "     Then $\\tilde f_X(x)$ is still a normal pdf since $P(X\\le x)=\\int_{-\\infty}^x f_X(x)dx=\\int_{-\\infty}^x \\tilde f_X(x)dx$.\n",
    "     \n",
    "     \n",
    "5. CLT: Given $X_1,\\dots,X_n\\sim i.i.d. F_{X}(\\cdot)$, if $E(X)< \\infty$, then $\\sqrt{n} (\\bar{X}_n-\\mu)\\xrightarrow{d}N(0, \\sigma^2)$\n",
    "\n",
    "    The CLT states that sum of independent small distubance is Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d285865",
   "metadata": {},
   "source": [
    "## Multivariate Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46531aad",
   "metadata": {},
   "source": [
    "Consider a multivariate normal r.v., $\\mathbb X\\sim N(\\mathbb\\mu, \\Sigma), \\mathbb X =(X_1, \\dots, X_p)^T$ where each $X_i$ is a univariate normal, i.e.,  $X_i\\sim N(\\mu_i, \\sigma_i^2=\\sigma_{ii})$\n",
    "\n",
    "Assume $\\Sigma$ is diagonal, i.e, $Cov(X_i,X_j)=0\\implies$ $X_i$ are independence (This is only true for jointly normal distribution).\n",
    "\n",
    "Then $f_{\\mathbb X}{(x_1, ..., x_p)}=f_{X_1}(x_1)\\times \\cdots \\times f_{X_p}(x_p)$\n",
    "\n",
    "$\\text{Proof:}$\n",
    "\n",
    "Consider p=2 for simplicity.\n",
    "\n",
    "$\\Sigma=\\begin{bmatrix}\\sigma_{11} & 0 \\\\ 0&\\sigma_{22}\\end{bmatrix}$\n",
    "\n",
    "$\\Sigma^{-1}=\\frac{1}{\\sigma_{11}\\sigma_{22}-0}\\begin{bmatrix}\\sigma_{22} & 0 \\\\ 0&\\sigma_{11}\\end{bmatrix}=\\begin{bmatrix}1/\\sigma_{11} & 0 \\\\ 0&1/\\sigma_{22}\\end{bmatrix}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{\\mathbb X}(x_1, x_2)&=\\frac{1}{\\sqrt{2\\pi}^p}(\\frac{1}{det(\\Sigma)})^{1/2}exp(-\\frac12 (\\mathbb x-\\mathbb\\mu)^T\\Sigma^{-1}(\\mathbb  x-\\mathbb \\mu))\\\\\n",
    "&=\\frac{1}{\\sqrt{2\\pi}^2}(\\frac{1}{\\sigma_{11}\\sigma_{22}})^{1/2}exp(-\\frac12 (\\mathbb x-\\mathbb\\mu)^T\\Sigma^{-1}(\\mathbb  x-\\mathbb \\mu))\\\\\n",
    "&=\\frac{1}{\\sqrt{2\\pi}^2}(\\frac{1}{\\sigma_{11}\\sigma_{22}})^{1/2}exp(-\\frac12 (\\frac{(x_1-\\mu_1)^2}{\\sigma_{11}}+\\frac{(x_2-\\mu_2)^2}{\\sigma_{22}}))\\\\\n",
    "&=f_{X_1(x_1)}f_{X_2(x_2)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e133de",
   "metadata": {},
   "source": [
    "Contour Plot:\n",
    "\n",
    "$f_{\\mathbb X}(x_1, x_2)=k\\iff \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}}+\\frac{(x_2-\\mu_2)^2}{\\sigma_{22}}=k$\n",
    "\n",
    "which is an ellipse in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f47633",
   "metadata": {},
   "source": [
    "## True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724832bd",
   "metadata": {},
   "source": [
    "1. Let $\\mathbb X=(X_1,...,X_p)^T\\sim N(\\mathbb\\mu, \\Sigma)$. If $X_1,X_2,··· ,X_p$ are mutually independent, $X_i ⊥ X_j,∀i \\ne j$. Are $X_1,X_2,··· ,X_p$ independent?\n",
    "\n",
    "   Yes, but this is Only True for jointly normal distribution since $X_i\\perp X_j\\implies Cov(X_i, X_j)=0\\implies \\Sigma\\text{ is diagonal}\\implies X_1, X_2, ..., X_p$ are independent\n",
    "\n",
    "\n",
    "\n",
    "2. Let $X_1, X_2$ be two normal random variables. Does $Cov(X_1, X_2) = 0$ imply $X_1 ⊥ X_2$?\n",
    "\n",
    "   No, This is only true for jointly normal distribution. Notice that $X_1, X_2$ be two normal random variables $\\ne$ $X_1,X_2$ jointly normal\n",
    " \n",
    "   An explanation is [here](https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent).\n",
    "   \n",
    "\n",
    "3. $X_1 , X_2$ are two normal random variable. Is $X_1 + X_2$ also normal?\n",
    "\n",
    "    No, again this is only true for jointly normal or you may need the independence assumption.\n",
    "\n",
    "    If the two normal random variables are not independent, then their sum is not necessarily normal.\n",
    "    \n",
    "    Definition: Two random variables $X$ and $Y$ are said to be bivariate normal, or jointly normal, if $aX+bY$ has a normal distribution for all $a,b∈ℝ$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d9cca",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e875b9",
   "metadata": {},
   "source": [
    "Show that if $Y=aX+b$ then $Corr(X,Y)=1$\n",
    "\n",
    "$\\text{Proof:}$\n",
    "\n",
    "Notice that $Cov(X, Y)=Cov(X, aX+b)=aVar(X)$ and $Var(Y)=a^2Var(X)$\n",
    "\n",
    "$$Corr(X, Y)=\\frac{Cov(X, Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}=\\frac{aVar(X)}{aVar(X)}=1$$\n",
    "\n",
    "$\\text{Remark: The converse is also true, i.e., }Corr(X,Y)=1\\implies \\exists a, b \\ni Y=aX+b$\n",
    "\n",
    "This also shows that covariance captures linear dependence (but not non-linear relationship)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6db0b2",
   "metadata": {},
   "source": [
    "## Applications of Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68224d",
   "metadata": {},
   "source": [
    "For example, Risk Hedging. Consider two stocks with returns $R_1, R_2$, with $E(R_1)=E(R_2)>0$, $Var(R_1)=Var(R_2)>0$, and $Cov(R_1, R_2)<0$\n",
    "\n",
    "Assume a portfolio $aR_1+bR_2$, with $a+b=1, 0<a<1, 0<b<1$\n",
    "\n",
    "Then $E(aR_1+bR_2)=aE(R_1)+bE(R_2)$ and $Var(aR_1+bR_2)=a^2Var(R_1)+b^2Var(R_2)+2ab\\underbrace{Cov(R_1,R_2)}_{<0}$\n",
    "\n",
    "Hence, the variance of the portfolio must be smaller than the individual stock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613a3eb",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2326bae",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e9538",
   "metadata": {},
   "source": [
    "project $\\vec a$ to $\\vec b$:  $\\frac{\\vec a\\cdot \\vec b}{||\\vec b||^2}\\vec b=\\frac{\\vec a\\cdot \\vec b}{\\vec b \\cdot \\vec b}\\vec b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8d311",
   "metadata": {},
   "source": [
    "Smaller reconstruction error $\\iff$ The projected variable has larger variance (less information loss)\n",
    "\n",
    "Given $\\mathbb X=(X_1,...,X_p)^T$ a random vector, we want to find a direction $a$ so that $Var(a^TX)$ is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b347bb",
   "metadata": {},
   "source": [
    "Consider $\\mathbb X=(X_1,...,X_p)^T$, $E(\\mathbb X)=\\mathbb 0$, $Cov(\\mathbb X)=\\Sigma$\n",
    "\n",
    "The first principal component (PC) $a_1$ is defined to be\n",
    "\n",
    "$$\\max_{a_1} Var(a_1^T\\mathbb X)\\\\\n",
    "s.t. ||a_1||=\\sqrt{a_1^Ta_1}=1$$\n",
    "\n",
    "The second PC is given by \n",
    "\n",
    "$$\\max_{a_2} Var(a_2^T\\mathbb X)\\\\\n",
    "s.t. ||a_2||=1, a_1^Ta_2=0 \\text{ (orthogonal)}$$\n",
    "\n",
    "\n",
    "The j-th PC, $j\\le p$ is given by \n",
    "\n",
    "$$\\max_{a_j}Var(a_j^T \\mathbb X)\\\\ \n",
    "s.t. ||a_j||=1, a_j^Ta_i=0, \\forall i<j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08fbf4",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21bbff4",
   "metadata": {},
   "source": [
    "Suppose Data has p dimensions, i.e., $x_i\\in \\mathbb R^p$\n",
    "\n",
    "Assume $E(x_i)=0$ (o.w., do demean)\n",
    "\n",
    "We want to find a function $f:\\mathbb R^p\\rightarrow \\mathbb R^q,q<p$ that encodes $x_i$ to $z_i=f(x_i)\\in \\mathbb R^q$\n",
    "\n",
    "Generally, we also want a decoder function $g(\\cdot):\\mathbb R^q\\rightarrow \\mathbb R^p$\n",
    "\n",
    "\n",
    "Goal: Minimize reconstruction error, i.e., \n",
    "\n",
    "$$\\min \\frac1n \\sum_{i=1}^{n}||x_i-g(f(x_i))||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c139c",
   "metadata": {},
   "source": [
    "For example, consider p=2, q=1, and $A_{2\\times1}=(a_1)$ \n",
    "\n",
    "Then for $x\\in \\mathbb R^2$\n",
    "\n",
    "we have $z= f(x)=A^Tx\\in \\mathbb R$ and \n",
    "\n",
    "$\\tilde x=g(z)=Az=AA^Tx\\in \\mathbb R^2$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1121d5",
   "metadata": {},
   "source": [
    "Now, to define projection for q>1, consider $A_{p\\times q}=(a_1,...,a_q)$\n",
    "\n",
    "Then for $x\\in \\mathbb R^p$\n",
    "\n",
    "we have $z= f(x)=A^Tx = \\begin{bmatrix}a_1^T\\\\\\vdots\\\\a_q^T\\end{bmatrix}x=\\begin{bmatrix}a_1^Tx\\\\\\vdots\\\\a_q^Tx\\end{bmatrix}\\in \\mathbb R^q$ \n",
    "\n",
    "Notice that since each $a^T_i$ is a $1\\times p$ vector and $x$ is a $p\\times 1$ vector, so $a^T_i x$ is a scalar for $i=1,...,q$\n",
    "\n",
    "$\\tilde x=g(z)=Az=AA^Tx = \\underbrace{(a_1,...,a_q)\\begin{bmatrix}a_1^Tx\\\\\\vdots\\\\a_q^Tx\\end{bmatrix}}_{\\in \\mathbb R^p} \\underbrace{=}_\\text{by matrix  multiplication} a_1 \\cdot a_1^Tx +... +a_q\\cdot a_q^Tx \\underbrace{=}_{commutative} \\underbrace{(a_1^Tx)  a_1}_{\\text{projection on }a_1} +...+\\underbrace{(a_q^T x)  a_q}_{\\text{projection on } a_q}$\n",
    "\n",
    "For the last eqaution, recall that $a^T_i x$ is a scalar for $i=1,...,q$ so commutative law is allowed.\n",
    "\n",
    "\n",
    "\n",
    "Lastly, recall project $\\vec a$ to $\\vec b$:  $\\frac{\\vec a\\cdot \\vec b}{||\\vec b||^2}\\vec b=\\frac{\\vec a\\cdot \\vec b}{\\vec b \\cdot \\vec b}\\vec b.$ So, if ||b||=1, it reduces to $(\\vec a \\cdot \\vec b) \\vec b=(\\vec a^T \\vec b) \\vec b $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21deff2e",
   "metadata": {},
   "source": [
    "Question:\n",
    "    \n",
    "Suppose p=3, q=2. What is matrix A for the projection on the plane $x=y$?\n",
    "\n",
    "Let $A=(a_1,a_2)$ What $a_1,a_2$ span the plane $x=y$?\n",
    "\n",
    "We first consider $A$ that projects onto the plane $z=0$\n",
    "\n",
    "i.e., $A=(a_1,a_2)=\\begin{bmatrix}1 & 0 \\\\ 0&1\\\\0&0\\end{bmatrix}$\n",
    "\n",
    "Then $AA^Tx = \\begin{bmatrix}1 & 0 \\\\ 0&1\\\\0&0\\end{bmatrix}\\begin{bmatrix}1&0&0\\\\0&1&0\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}1 & 0 \\\\ 0&1\\\\0&0\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}$\n",
    "\n",
    "This A projects to xy-plane.\n",
    "\n",
    "Lastly, we need $x = y$, consider \n",
    "\n",
    "$A =(a_1,a_2)=\\begin{bmatrix}1/\\sqrt{2} & 0 \\\\ 1/\\sqrt{2}&0\\\\0&1\\end{bmatrix}$\n",
    "\n",
    "Then $AA^Tx = \\begin{bmatrix}1/\\sqrt{2} & 0 \\\\ 1/\\sqrt{2}&0\\\\0&1\\end{bmatrix}\\begin{bmatrix}1/\\sqrt{2}&1/\\sqrt{2}&0\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}1/\\sqrt{2} & 0 \\\\ 1/\\sqrt{2}&0\\\\0&1\\end{bmatrix}\\begin{bmatrix}1/\\sqrt{2}x+1/\\sqrt{2}y\\\\z\\end{bmatrix}=\\begin{bmatrix}\\frac12 x+\\frac12 y\\\\\\frac12 x+\\frac12 y\\\\z\\end{bmatrix}$\n",
    "\n",
    "\n",
    "You see now the x-coordinate and y-coodinate equals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dabf160",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac9d02",
   "metadata": {},
   "source": [
    "In fact,\n",
    "\n",
    "\n",
    "Notice that $||x||^2\\underbrace{=}_\\text{Pythagorean Thm}||x-a_1a_1^Tx||^2+||a_1a_1^Tx||^2$\n",
    "\n",
    "\n",
    "Hence $\\min ||x-a_1a_1^Tx||^2 \\iff \\max ||a_1a_1^Tx||^2 \\iff \\max Var(a_1^Tx)$\n",
    "\n",
    "To see the last one, notice two facts:\n",
    "\n",
    "\n",
    "1. $\\frac1n \\sum ||a_1a_1^Tx||^2 \\underbrace{=}_{a_1^Tx\\text{ is scalar }}\\frac1n \\sum (a_1^Tx)^2||a_1||^2 \\underbrace{=}_{||a_1||=1} \\frac1n \\sum (a_1^Tx)^2\\xrightarrow{p} E[(a_1^Tx)^2]$ as $n\\rightarrow \\infty$\n",
    "\n",
    "2. $E[(a_1^Tx)^2] = E[(a_1^Tx)(a_1^Tx)] = E[(a_1^Tx)(x^Ta_1)]=E[a_1^Txx^Ta_1] = a_1^TE(xx^T)a_1\\underbrace{=}_{\\because E(x)=0} a_1^TCov(x)a_1=Var(a_1^Tx)= a_1^T\\Sigma a_1 \\in \\mathbb R$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3109e87",
   "metadata": {},
   "source": [
    "If we assume $\\Sigma=\\begin{bmatrix}\\lambda_1 &0&\\dots&0\\\\0&\\lambda_2&\\dots& 0\\\\\n",
    "\\vdots& \\vdots&\\ddots&\\vdots\\\\0&0&0&\\lambda_p \\end{bmatrix}$ and $\\lambda_1>\\lambda_2>\\dots>\\lambda_p$\n",
    "\n",
    "Then it's easy to see that the solution to \n",
    "\n",
    "$$\\max_{a_1}Var(a_1^Tx)\\\\\n",
    "s.t. ||a_1||=1$$\n",
    "\n",
    "is $a_1^*=(1,0,...,0)$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
